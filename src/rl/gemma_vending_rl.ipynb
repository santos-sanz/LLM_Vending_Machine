# %% [markdown]
# # Reinforcement Learning for Vending Machine Profit Optimization
# 
# This notebook implements a complete RL pipeline using GRPO (Group Relative Policy Optimization) to train a Gemma 3 (1B) model to maximize vending machine profits.
# 
# **Goal**: Train the model to make strategic pricing decisions.
# **Reward**: The weekly profit generated by the simulator based on the model's decisions.
# **Verifier**: Checks if the model output is valid JSON.

# %% [markdown]
# ## 1. Installation & Setup
# Unsloth is used for efficient training.

# %%
# %%
# %%capture
# # Unsloth specific installation for Colab and Kaggle
# !pip install unsloth vllm
# !pip install --upgrade uv
# !uv pip install unsloth vllm numpy pillow torchvision bitsandbytes xformers trl transformers datasets

# # If running on Kaggle, you might need:
# # !pip install "unsloth[kaggle-new]" @ git+https://github.com/unslothai/unsloth.git

# %% [markdown]
# ## 2. Core Simulation Classes
# We define the Vending Machine environment here so this notebook is self-contained.

# %%
import random
import numpy as np
import json
import re
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple, Set

# --- Product Class ---
@dataclass
class Product:
    name: str
    price: float
    cost: float
    stock: int
    max_stock: int
    base_likelihood: float
    price_sensitivity: float

    @property
    def purchase_likelihood(self) -> float:
        # Calculate likelihood: base likelihood minus price * sensitivity
        calculated_likelihood = self.base_likelihood - (self.price * self.price_sensitivity)
        return max(0.0, min(1.0, calculated_likelihood))

    def __repr__(self):
        return f"Product(name='{self.name}', price={self.price:.2f}, stock={self.stock}/{self.max_stock})"

# --- Vending Machine Class ---
class VendingMachine:
    def __init__(self, initial_cash: float = 0.0, maintenance_cost: float = 20.0, initial_investment: float = 0.0):
        self.products: Dict[str, Product] = {}
        self.cash = initial_cash
        self.maintenance_cost = maintenance_cost
        self.initial_investment = initial_investment

    def add_product(self, config: dict):
        p = Product(**config)
        if p.name in self.products:
            existing = self.products[p.name]
            self.initial_investment += (p.stock - existing.stock) * p.cost
            self.products[p.name] = p
        else:
            self.products[p.name] = p
            self.initial_investment += p.stock * p.cost

    def get_product(self, name: str) -> Optional[Product]:
        return self.products.get(name)

    def recharge_products(self):
        for name, p in self.products.items():
            needed = p.max_stock - p.stock
            if needed > 0:
                cost = needed * p.cost
                self.cash -= cost
                p.stock = p.max_stock

    def sell_product(self, name: str) -> bool:
        p = self.products.get(name)
        if p and p.stock > 0:
            p.stock -= 1
            self.cash += p.price
            return True
        return False

    def apply_maintenance(self):
        self.cash -= self.maintenance_cost

    def calculate_profit_loss(self) -> float:
        return self.cash - self.initial_investment

# %% [markdown]
# ## 3. Simulation Engine
# Logic to simulate one week of competition.

# %%
# --- Configuration ---
PRODUCT_CONFIGS = [
    {"name": "Soda",      "price": 2.50, "cost": 1.00, "stock": 20, "max_stock": 20, "base_likelihood": 0.725, "price_sensitivity": 0.05},
    {"name": "Chips",     "price": 1.75, "cost": 0.75, "stock": 30, "max_stock": 30, "base_likelihood": 0.54,  "price_sensitivity": 0.08},
    {"name": "Candy Bar", "price": 1.25, "cost": 0.50, "stock": 40, "max_stock": 40, "base_likelihood": 0.75,  "price_sensitivity": 0.04},
    {"name": "Water",     "price": 1.00, "cost": 0.30, "stock": 50, "max_stock": 50, "base_likelihood": 0.83,  "price_sensitivity": 0.03}
]
CLIENT_LAMBDA = 100 # Average clients per week

def simulate_competitive_week(machines: List[VendingMachine], verbose=False) -> Tuple[List[float], List[str]]:
    """Simulates one week of purchases and returns profits and stockout events."""
    stockout_events = []
    reported_stockouts = set()

    for vm in machines:
        vm.recharge_products()
        vm.apply_maintenance()

    # Poisson distributed clients
    num_clients = np.random.poisson(CLIENT_LAMBDA)
    
    for client_idx in range(num_clients):
        # 1. Check for new stockouts to report
        for i, vm in enumerate(machines):
            for p_name, p in vm.products.items():
                if p.stock == 0:
                    event_key = (i, p_name)
                    if event_key not in reported_stockouts:
                        msg = f"Machine {i}: {p_name} is out of stock!"
                        stockout_events.append(msg)
                        reported_stockouts.add(event_key)
        
        # 2. Gather options
        options = []
        for vm in machines:
            for p in vm.products.values():
                if p.stock > 0:
                    options.append((vm, p.name, p.purchase_likelihood))
        
        if not options:
            break
            
        # 3. Client Choice
        vms, names, likelihoods = zip(*options)
        total_likelihood = sum(likelihoods)
        
        if total_likelihood <= 0:
            continue
            
        probs = [l / total_likelihood for l in likelihoods]
        chosen_idx = np.random.choice(len(options), p=probs)
        chosen_vm, chosen_name, _ = options[chosen_idx]
        
        chosen_vm.sell_product(chosen_name)
        
    return [vm.calculate_profit_loss() for vm in machines], stockout_events

# %% [markdown]
# ## 4. Training Data Generation
# We generate synthetic scenarios (prompts) to train the model.

# %%
from datasets import Dataset

def generate_random_market_data():
    return {
        "competitor_prices": {p["name"]: round(p["price"] * random.uniform(0.8, 1.2), 2) for p in PRODUCT_CONFIGS},
        "estimated_demand": random.choice(["Low", "Medium", "High"]),
        "marketing_intensity": random.choice(["Low", "Medium", "High"]),
        "upcoming_events": random.choice(["None", "local_festival", "school_holiday"])
    }

def generate_random_stockout_events():
    if random.random() < 0.7:
        return "No products ran out of stock this week."
    events = []
    num_events = random.randint(1, 3)
    for _ in range(num_events):
        prod = random.choice(PRODUCT_CONFIGS)["name"]
        events.append(f"Machine 0: {prod} is out of stock!")
    return "\n".join(events)

def generate_prompt(idx):
    market_data = generate_random_market_data()
    event_feedback = generate_random_stockout_events()
    week_num = random.randint(1, 10)
    
    prompt = f"""
    You are a Strategic Business Manager for 'LLMMachine'. You are in direct competition with 'BasicMachine'.
    Both machines share the same client pool. Clients choose the product with the highest 'purchase_likelihood'.

    IMPORTANT RULES:
    1. AT THE START OF EACH WEEK, YOUR MACHINE IS REFILLED TO MAX CAPACITY.
    2. 'BasicMachine' NEVER changes its prices.
    3. Your goal is to maximize NET PROFIT.

    STRATEGIC ADVICE:
    - Your priority is PROFIT, not just sales volume.
    - Selling out is NOT always the goal. If there is more supply than demand, you might never sell out.
    - If you raise prices, you might sell fewer items, but if the margin increase covers the volume loss, your PROFIT goes up.
    
    AVAILABLE TOOLS (Respond only in JSON):
    1. {{"action": "change_price", "parameters": {{"machine_name": "LLMMachine", "product_name": "...", "new_price": ...}}}}
    2. {{"action": "next_week"}} - to proceed.
    
    Provide your reasoning first, then the JSON block.

    --- WEEK {week_num} RESULTS ---
    Stockout Events:
    {event_feedback}
    
    Current Market Data:
    {json.dumps(market_data, indent=2)}
    
    Machines are being refilled NOW. What adjustments will you make for Week {week_num+1}?
    """
    return {"prompt": prompt}

# Generate Dataset
dataset = Dataset.from_list([generate_prompt(i) for i in range(100)])

# %% [markdown]
# ## 5. Model Loading & GRPO Trainer
# We use Unsloth to load Gemma 3 and train it.

# %%
import os
import torch
from unsloth import FastModel
from trl import GRPOConfig, GRPOTrainer

# Configuration
MAX_SEQ_LENGTH = 1024
LORA_RANK = 8

# Load Env Vars (Colab & Kaggle support)
try:
    from google.colab import userdata
    hf_token = userdata.get('HUGGINGFACE_TOKEN')
except ImportError:
    try:
        from kaggle_secrets import UserSecretsClient
        user_secrets = UserSecretsClient()
        hf_token = user_secrets.get_secret("HUGGINGFACE_TOKEN")
    except ImportError:
        hf_token = os.getenv("HUGGINGFACE_TOKEN")

model_name = "unsloth/gemma-3-1b-it"

model, tokenizer = FastModel.from_pretrained(
    model_name=model_name,
    max_seq_length=MAX_SEQ_LENGTH,
    load_in_4bit=False, 
    load_in_8bit=False,
    full_finetuning=False,
    token=hf_token,
)

model = FastModel.get_peft_model(
    model,
    r=LORA_RANK,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=LORA_RANK,
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

# %% [markdown]
# ## 6. Reward Functions
# 
# 1. **Format Verifier**: Ensures the output contains valid JSON with an "action" key.
# 2. **Profit Reward**: Runs the simulation with the model's chosen action and returns the profit.

# %%
def extract_json_action(text):
    try:
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            return json.loads(match.group(0))
    except:
        pass
    return None

def format_reward_func(prompts, completions, **kwargs):
    rewards = []
    for completion in completions:
        response_text = completion[0]["content"]
        action_json = extract_json_action(response_text)
        if action_json and "action" in action_json:
            rewards.append(1.0)
        else:
            rewards.append(0.0)
    return rewards

def profit_reward_func(prompts, completions, **kwargs):
    rewards = []
    for completion in completions:
        response_text = completion[0]["content"]
        action_json = extract_json_action(response_text)
        
        if not action_json:
            rewards.append(-5.0) # Heavy penalty for no JSON
            continue

        # Setup Simulation
        basic_machine = VendingMachine(initial_cash=0)
        llm_machine = VendingMachine(initial_cash=0)
        
        # Load Products
        for config in PRODUCT_CONFIGS:
            basic_machine.add_product(config)
            llm_machine.add_product(config)

        # Apply Action
        valid_action = False
        if action_json.get("action") == "change_price":
            params = action_json.get("parameters", {})
            p_name = params.get("product_name")
            new_price = params.get("new_price")
            if p_name and new_price is not None:
                prod = llm_machine.get_product(p_name)
                if prod:
                    prod.price = float(new_price)
                    valid_action = True
        elif action_json.get("action") == "next_week":
             valid_action = True

        if not valid_action:
             rewards.append(-1.0) # Invalid action parameters
             continue
             
        # Run Simulation
        try:
            simulate_competitive_week([basic_machine, llm_machine])
            profit = llm_machine.calculate_profit_loss()
            rewards.append(profit)
        except Exception as e:
             # Simulation crash
            rewards.append(-5.0)

    return rewards

# %% [markdown]
# ## 7. Start Training

# %%
training_args = GRPOConfig(
    output_dir="outputs/gemma_rl_vending",
    learning_rate=1e-6,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_generations=4,
    max_prompt_length=512,
    max_completion_length=200,
    max_steps=100, 
    save_steps=50,
    report_to="none",
)

trainer = GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    reward_funcs=[format_reward_func, profit_reward_func],
    args=training_args,
    train_dataset=dataset,
)

print("Starting Training...")
trainer.train()
print("Training Complete!")

# %%
# Save Model
model.save_pretrained("gemma-3-vending-rl")
tokenizer.save_pretrained("gemma-3-vending-rl")
